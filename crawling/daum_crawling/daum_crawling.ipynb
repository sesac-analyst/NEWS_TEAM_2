{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "# 카테고리 ID 설정\n",
    "category_id = 'IT'\n",
    "\n",
    "# 날짜 범위 설정\n",
    "start_date = datetime(2023, 9, 1)\n",
    "end_date = datetime(2024, 9, 1)\n",
    "\n",
    "def fetch_page_urls(date_str, page):\n",
    "    url = f'https://news.daum.net/breakingnews/digital?page={page}&regDate={date_str}'\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        return [], True\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # 뉴스 리스트 엘리먼트 찾기\n",
    "    news_list = soup.find_all('ul', class_='list_news2 list_allnews')\n",
    "    \n",
    "    if not news_list:\n",
    "        return [], True  # 뉴스 리스트가 없으면 종료\n",
    "    \n",
    "    page_urls = []\n",
    "    for news_section in news_list:\n",
    "        news_items = news_section.find_all('li')\n",
    "        for item in news_items:\n",
    "            link = item.find('a', class_='link_txt')['href']\n",
    "            page_urls.append(link)\n",
    "    \n",
    "    return page_urls, False\n",
    "\n",
    "def collect_urls_for_date(date):\n",
    "    date_str = date.strftime('%Y%m%d')\n",
    "    page = 1\n",
    "    all_urls = []\n",
    "    previous_page_urls = set()\n",
    "\n",
    "    while True:\n",
    "        page_urls, is_end = fetch_page_urls(date_str, page)\n",
    "        if is_end or not page_urls or set(page_urls) == previous_page_urls:\n",
    "            break\n",
    "        \n",
    "        previous_page_urls = set(page_urls)\n",
    "        all_urls.extend(page_urls)\n",
    "        page += 1\n",
    "    \n",
    "    return all_urls\n",
    "\n",
    "def collect_urls_for_range(start_date, end_date):\n",
    "    current_date = start_date\n",
    "    all_urls = []\n",
    "    total_days = (end_date - start_date).days + 1\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(collect_urls_for_date, current_date + timedelta(days=i)): (current_date + timedelta(days=i)).strftime('%Y%m%d') for i in range(total_days)}\n",
    "        \n",
    "        with tqdm(total=total_days, desc='전체 URL 수집 진행 상황', leave=True) as date_bar:\n",
    "            for future in as_completed(futures):\n",
    "                date_str = futures[future]\n",
    "                try:\n",
    "                    daily_urls = future.result()\n",
    "                    all_urls.extend(daily_urls)\n",
    "                except Exception as e:\n",
    "                    print(f'{date_str}에서 URL 수집 중 오류 발생: {e}')\n",
    "                date_bar.update(1)\n",
    "                \n",
    "    return all_urls\n",
    "\n",
    "def fetch_article_data(url):\n",
    "    try:\n",
    "        time.sleep(0.3)  # 요청 간 지연\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # 제목 크롤링\n",
    "        title_tag = soup.find('h3', class_='tit_view')\n",
    "        title = title_tag.get_text() if title_tag else 'Null'\n",
    "        \n",
    "        # 작성자와 날짜 크롤링\n",
    "        author_tag = soup.find('span', class_='txt_info')\n",
    "        author = author_tag.get_text() if author_tag else 'Null'\n",
    "        date_tag = soup.find('span', class_='num_date')\n",
    "        publication_date = date_tag.get_text() if date_tag else 'Null'\n",
    "        \n",
    "        # 본문 크롤링 (p 태그에서 text만 추출, 특정 클래스의 p 태그 제외)\n",
    "        paragraphs = soup.find_all('p')\n",
    "        content = ''\n",
    "        for paragraph in paragraphs:\n",
    "            if 'class' not in paragraph.attrs:\n",
    "                content += paragraph.get_text() + ' '\n",
    "        \n",
    "        # 출처 크롤링\n",
    "        publisher_id_tag = soup.find('a', {'data-tiara-action-name': 'GNB언론사명_클릭'})\n",
    "        publisher_id = publisher_id_tag.get_text() if publisher_id_tag else 'Null'\n",
    "\n",
    "        # 데이터 저장\n",
    "        return {\n",
    "            'article_url': url,\n",
    "            'title': title.strip(),\n",
    "            'author': author.strip(),\n",
    "            'publication_date': publication_date.strip(),\n",
    "            'content': content.strip(),\n",
    "            'publisher_id': publisher_id.strip(),\n",
    "            'category_id': category_id,\n",
    "            'platform_id': '다음'  # 플랫폼 ID를 '다음'으로 설정\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f'기사 데이터 크롤링 중 오류 발생: {e}')\n",
    "        return None\n",
    "\n",
    "def collect_article_data(urls):\n",
    "    results = []\n",
    "    total_urls = len(urls)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(fetch_article_data, url): url for url in urls}\n",
    "        \n",
    "        with tqdm(total=total_urls, desc='기사 데이터 크롤링 진행 상황', leave=True) as url_bar:\n",
    "            for future in as_completed(futures):\n",
    "                url = futures[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f'{url}에서 기사 크롤링 중 오류 발생: {e}')\n",
    "                url_bar.update(1)\n",
    "\n",
    "    return results\n",
    "\n",
    "# URL 수집 및 기사 데이터 크롤링\n",
    "print(\"URL 수집 시작...\")\n",
    "urls = collect_urls_for_range(start_date, end_date)\n",
    "print(f\"총 {len(urls)}개의 URL 수집됨.\")\n",
    "\n",
    "print(\"기사 데이터 크롤링 시작...\")\n",
    "results = collect_article_data(urls)\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_file = \"news_data_range.csv\"\n",
    "csv_columns = ['article_url', 'title', 'author', 'publication_date', 'content', 'publisher_id', 'category_id', 'platform_id']\n",
    "\n",
    "try:\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "        writer.writeheader()\n",
    "        with tqdm(total=len(results), desc='CSV 파일 저장 진행 상황', leave=True) as pbar:\n",
    "            for row in results:\n",
    "                writer.writerow(row)\n",
    "                pbar.update(1)\n",
    "except IOError:\n",
    "    print(\"I/O error\")\n",
    "\n",
    "print(f\"크롤링 완료. '{csv_file}'에 데이터 저장됨.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
