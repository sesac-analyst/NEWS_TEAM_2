{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 뉴스 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import concurrent.futures\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# 날짜 범위 설정\n",
    "start_date = datetime.strptime(\"20230901\", \"%Y%m%d\")\n",
    "end_date = datetime.strptime(\"20240901\", \"%Y%m%d\")\n",
    "\n",
    "# 날짜 리스트 생성\n",
    "date_list = [(start_date + timedelta(days=x)).strftime(\"%Y%m%d\") for x in range((end_date - start_date).days + 1)]\n",
    "\n",
    "# 장르별 section_id 리스트 생성\n",
    "section_ids = ['731', '226', '227', '732', '283', '229', '228', '230']\n",
    "\n",
    "# section_id 리스트 네임\n",
    "    # 731 : 모바일 \n",
    "    # 226 : 인터넷/SNS \n",
    "    # 227 : 통신/뉴미디어 \n",
    "    # 732 : 보안/해킹 \n",
    "    # 283 : 컴퓨터 \n",
    "    # 229 : 게임/리뷰 \n",
    "    # 228 : 과학 일반 \n",
    "    # 230 : IT 일반\n",
    "\n",
    "# 함수: 개별 URL에서 제목, 기자 이름 등을 가져오기\n",
    "def crawl_article(url):\n",
    "    result = {\n",
    "        'url': url,\n",
    "        'title': None,\n",
    "        'author': None,\n",
    "        'publication_date': None,\n",
    "        'update_date': None,\n",
    "        'publisher_id': 'Null',  # 기본값 설정\n",
    "        'category_id': 'IT',  # 고정 값\n",
    "        'platform_id': '네이버',  # 고정 값\n",
    "        'content': 'Null'  # 기본값 설정\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        article_response = requests.get(url)\n",
    "        article_response.raise_for_status()\n",
    "        \n",
    "        article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
    "\n",
    "        # 제목 가져오기\n",
    "        title_tag = article_soup.find('h2', class_='media_end_head_headline')\n",
    "        result['title'] = title_tag.get_text(strip=True) if title_tag else 'Null'\n",
    "        \n",
    "        # 기자 이름 가져오기 (우선 <em> 태그에서 가져오기)\n",
    "        author_tag = article_soup.find('em', class_='media_end_head_journalist_name')\n",
    "        if author_tag:\n",
    "            result['author'] = author_tag.get_text(strip=True)\n",
    "        else:\n",
    "            # <div class=\"byline\"> 태그에서 가져오기\n",
    "            byline_tag = article_soup.find('div', class_='byline')\n",
    "            if byline_tag:\n",
    "                byline_text = byline_tag.get_text(strip=True)\n",
    "                if '기자' in byline_text:\n",
    "                    result['author'] = byline_text.split(' 기자')[0]\n",
    "                else:\n",
    "                    result['author'] = 'Null'\n",
    "            else:\n",
    "                result['author'] = 'Null'\n",
    "        \n",
    "        # publication_date 가져오기\n",
    "        pub_date_tag = article_soup.find('div', class_='media_end_head_info_datestamp_bunch')\n",
    "        result['publication_date'] = pub_date_tag.find('span').get_text(strip=True) if pub_date_tag else 'Null'\n",
    "\n",
    "        # update_date 가져오기\n",
    "        datestamp_bunch = article_soup.find_all('div', class_='media_end_head_info_datestamp_bunch')\n",
    "        if len(datestamp_bunch) > 1:\n",
    "            result['update_date'] = datestamp_bunch[1].find('span').get_text(strip=True)\n",
    "        else:\n",
    "            result['update_date'] = 'Null'\n",
    "        \n",
    "        # publisher_id 가져오기\n",
    "        publisher_tag = article_soup.find('div', class_='media_end_head_top')\n",
    "        if publisher_tag:\n",
    "            img_tag = publisher_tag.find('img', class_='media_end_head_top_logo_img')\n",
    "            result['publisher_id'] = img_tag['alt'] if img_tag and 'alt' in img_tag.attrs else 'Null'\n",
    "        else:\n",
    "            result['publisher_id'] = 'Null'\n",
    "\n",
    "        # content 가져오기\n",
    "        content_tag = article_soup.find('article', id='dic_area')\n",
    "        result['content'] = content_tag.get_text(strip=True) if content_tag else 'Null'\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to crawl {url}: {e}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 멀티스레딩을 이용해 각 URL에서 데이터를 크롤링\n",
    "def process_date(date):\n",
    "    all_results = []\n",
    "    for section_id in section_ids:\n",
    "        base_url = f\"https://news.naver.com/breakingnews/section/105/{section_id}?date={date}\"\n",
    "        response = requests.get(base_url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # URL 크롤링: 모든 <a href> 태그의 링크를 가져오기\n",
    "        urls = []\n",
    "        articles = soup.find_all('div', class_='sa_text')\n",
    "        for article in articles:\n",
    "            link_tag = article.find('a', class_='sa_text_title')\n",
    "            if link_tag and link_tag.has_attr('href'):\n",
    "                urls.append(link_tag['href'])\n",
    "\n",
    "        # 각 URL에 대해 크롤링 수행\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:\n",
    "            # tqdm 사용하여 크롤링 진행 상황 표시\n",
    "            for result in tqdm(executor.map(crawl_article, urls), desc=f'Processing articles on {date}, section {section_id}', total=len(urls), unit='article', dynamic_ncols=True, ascii=True):\n",
    "                all_results.append(result)\n",
    "        \n",
    "        time.sleep(0.3)  # 각 section 별로 딜레이 추가\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# 날짜별로 크롤링 진행 및 CSV 파일로 저장\n",
    "all_results = []\n",
    "for date in tqdm(date_list, desc='Processing dates', unit='date', dynamic_ncols=True, ascii=True):\n",
    "    daily_results = process_date(date)\n",
    "    all_results.extend(daily_results)\n",
    "\n",
    "# DataFrame으로 변환하여 CSV 파일로 저장\n",
    "df = pd.DataFrame(all_results)\n",
    "df.to_csv('naver_news_crawled_data.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"크롤링이 완료되었고, 데이터가 'naver_news_crawled_data.csv' 파일에 저장되었습니다.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
