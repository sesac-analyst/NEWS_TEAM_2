{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kefvnzoDy-8D",
        "outputId": "d76eb79e-d3fc-443d-fe65-cc2b4dee4919"
      },
      "outputs": [],
      "source": [
        "# !pip3 install kobert-transformers\n",
        "# !pip3 install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n",
        "# !pip3 install gensim\n",
        "# !pip3 install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sFFuAyYDx352"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "from transformers import BertTokenizer, BertModel, DistilBertModel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_naver = pd.read_csv('../data/filtered_naver.csv')\n",
        "df_daum = pd.read_csv('../data/filtered_daum.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_naver = df_naver[['url', 'title', 'publication_date', 'content', 'platform_id', 'entities']]\n",
        "df_daum = df_daum[['article_url', 'title', 'publication_date', 'content', 'platform_id', 'entities']]\n",
        "df_daum.rename(columns={'article_url': 'url'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 86970 entries, 0 to 86969\n",
            "Data columns (total 6 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   url               86970 non-null  object\n",
            " 1   title             86970 non-null  object\n",
            " 2   publication_date  86970 non-null  object\n",
            " 3   content           86970 non-null  object\n",
            " 4   platform_id       86970 non-null  object\n",
            " 5   entities          86970 non-null  object\n",
            "dtypes: object(6)\n",
            "memory usage: 4.0+ MB\n"
          ]
        }
      ],
      "source": [
        "df = pd.concat([df_naver, df_daum], ignore_index=True)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['entities'] = df['entities'].apply(lambda x: ast.literal_eval(x))\n",
        "# print(ast.literal_eval(df['entities'].iloc[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "x4ZwxyUWz4Bs"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8644, 100)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Using Word2Vec to train extracted\n",
        "model = Word2Vec(sentences=df['entities'], vector_size=100, window = 5, min_count=5, workers=1, sg=0)\n",
        "model.wv.vectors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gZ4xsQ1kFSW4"
      },
      "outputs": [],
      "source": [
        "# Function to vectorize a single sentence\n",
        "def vectorize_sentence(sentence, model):\n",
        "    # Filter words that are in the model's vocabulary\n",
        "    words_in_vocab = [word for word in sentence if word in model.wv]\n",
        "    if not words_in_vocab:\n",
        "        return np.zeros(model.vector_size)  # Return a zero vector if no words are in the vocabulary\n",
        "    # Compute the average of the word vectors\n",
        "    return np.mean([model.wv[word] for word in words_in_vocab], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ljdtg909L3l8"
      },
      "outputs": [],
      "source": [
        "# Vectorize all sentences in the DataFrame\n",
        "df['sentence_vector'] = df['entities'].apply(lambda sentence: vectorize_sentence(sentence, model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TRFIwhdfL7TH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.04816673  0.2127186  -0.01453179 -0.03493057  0.03977986 -0.49994612\n",
            "  0.19581053  0.46102628 -0.26858893 -0.20721993 -0.10559356 -0.28797114\n",
            " -0.21497738  0.01436195  0.09616459 -0.26236767 -0.01631447 -0.32519728\n",
            "  0.08266761 -0.4751123   0.23728502  0.10821801  0.30702502 -0.22461262\n",
            "  0.04133242 -0.01995973 -0.08435918  0.03396496 -0.32426602 -0.10741277\n",
            "  0.41913795 -0.03126465  0.18564647 -0.2999481  -0.02468124  0.2415332\n",
            "  0.06063355 -0.26852983 -0.22428298 -0.31046468  0.03947989 -0.31375143\n",
            " -0.11042176  0.03847359  0.18404472 -0.11118976 -0.27571586 -0.017821\n",
            "  0.01570749  0.25077862  0.15949066 -0.14106984 -0.03735182  0.05808438\n",
            " -0.1240892   0.2094386   0.22478369  0.1293802  -0.1398439   0.05922464\n",
            "  0.0210564   0.17840818 -0.03111822 -0.04661287 -0.2330937   0.20819831\n",
            " -0.0195927   0.265367   -0.20482512  0.19415417 -0.24649572  0.29874712\n",
            "  0.21585374 -0.08399407  0.3153918   0.04311277 -0.08530561  0.11351166\n",
            " -0.22085941 -0.09046302 -0.20125493 -0.14566287 -0.16449377  0.318093\n",
            " -0.08658781  0.07475922  0.11799282  0.268455    0.33252138  0.12169151\n",
            "  0.23273328  0.10974918 -0.03421774 -0.08911107  0.4539373   0.23647228\n",
            "  0.22501512 -0.25913975  0.14567757  0.01585001]\n"
          ]
        }
      ],
      "source": [
        "# Example: Access the vector for the first sentence\n",
        "first_sentence_vector = df['sentence_vector'].iloc[0]\n",
        "print(first_sentence_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
            "The class this function is called from is 'BertTokenizer'.\n",
            "c:\\Users\\s_torileeo99\\AppData\\Local\\anaconda3\\envs\\trans\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
        "model = BertModel.from_pretrained('monologg/kobert')\n",
        "# model = DistilBertModel.from_pretrained('monologg/distilkobert')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to get a single vector from BERT embeddings (e.g., mean pooling)\n",
        "def get_pooled_bert_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}  # Move inputs to GPU\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # Mean pooling over the sequence length dimension\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "GPU Name: NVIDIA GeForce GTX 1660 SUPER\n"
          ]
        }
      ],
      "source": [
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sample = df.sample(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get embeddings for each row\n",
        "df_sample['content_vector'] = df_sample['content'].apply(get_pooled_bert_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_sample.to_csv('../data/embedded_sample.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom Dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.dataframe.iloc[idx]['content']\n",
        "        inputs = self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
        "        return {key: value.squeeze(0) for key, value in inputs.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
            "The class this function is called from is 'BertTokenizer'.\n",
            "c:\\Users\\s_torileeo99\\AppData\\Local\\anaconda3\\envs\\trans\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
        "model = BertModel.from_pretrained('monologg/kobert')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset and dataloader\n",
        "dataset = TextDataset(df, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to get pooled BERT embeddings for a batch\n",
        "def get_batch_embeddings(batch):\n",
        "    inputs = {key: value.to(device) for key, value in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # Mean pooling over the sequence length dimension\n",
        "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process the data in batches\n",
        "content_vectors = []\n",
        "for batch in dataloader:\n",
        "    batch_embeddings = get_batch_embeddings(batch)\n",
        "    content_vectors.extend(batch_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['content_vector'] = content_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['embedding'] = df.apply(lambda row: np.concatenate((row['sentence_vector'], row['content_vector'])), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 86970 entries, 0 to 86969\n",
            "Data columns (total 9 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   url               86970 non-null  object\n",
            " 1   title             86970 non-null  object\n",
            " 2   publication_date  86970 non-null  object\n",
            " 3   content           86970 non-null  object\n",
            " 4   platform_id       86970 non-null  object\n",
            " 5   entities          86970 non-null  object\n",
            " 6   sentence_vector   86970 non-null  object\n",
            " 7   content_vector    86970 non-null  object\n",
            " 8   embedding         86970 non-null  object\n",
            "dtypes: object(9)\n",
            "memory usage: 6.0+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv('../data/embedded.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
