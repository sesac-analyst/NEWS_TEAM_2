{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2022530,"status":"ok","timestamp":1725792490639,"user":{"displayName":"이종찬 (다차원정신상태)","userId":"12094448046111944300"},"user_tz":-540},"id":"895Juqjf7ZXZ","outputId":"b89c0fa5-650c-44a6-90ff-3b8d043ad575"},"outputs":[],"source":["import pandas as pd\n","import re\n","import nltk\n","from transformers import BertTokenizer\n","from nltk.corpus import stopwords\n","from google.colab import drive\n","\n","# NLTK stopwords 다운로드\n","nltk.download('stopwords')\n","\n","# BERT 토크나이저 초기화\n","tokenizer = BertTokenizer.from_pretrained('kykim/bert-kor-base')\n","\n","# Google Drive 마운트\n","drive.mount('/content/drive')\n","\n","# 1. CSV 파일 불러오기\n","df = pd.read_csv('/content/drive/MyDrive/Team2/daum_news_data.csv')\n","\n","# 2. 결측값 처리 (빈 문자열로 대체)\n","df['content'] = df['content'].fillna('')\n","df['author'] = df['author'].fillna('')\n","df['title'] = df['title'].fillna('')\n","\n","# 3. content, title, author 클렌징\n","\n","# 4. 숫자 제거 (content, title, author)\n","df['content'] = df['content'].str.replace(r'\\d+', '', regex=True)\n","df['title'] = df['title'].str.replace(r'\\d+', '', regex=True)\n","df['author'] = df['author'].str.replace(r'\\d+', '', regex=True)\n","\n","# 5. 특수문자 및 한문 제거 (content, title, author)\n","df['content'] = df['content'].apply(lambda x: re.sub(r'[^\\w\\s]|[\\u4e00-\\u9fff]', '', x))\n","df['title'] = df['title'].apply(lambda x: re.sub(r'[^\\w\\s]|[\\u4e00-\\u9fff]', '', x))\n","df['author'] = df['author'].apply(lambda x: re.sub(r'[^\\w\\s]|[\\u4e00-\\u9fff]', '', x))\n","\n","# 6. 중복 데이터 제거 (content, title 기준)\n","df = df.drop_duplicates(subset=['content', 'title'])\n","\n","# 7. 불용어 제거 (NLTK와 BERT를 사용)\n","stop_words = set(stopwords.words('english'))  # NLTK의 영어 불용어 사용\n","\n","def remove_stopwords(text):\n","    tokens = tokenizer.tokenize(text)  # BERT 토크나이저로 형태소 분석\n","    return ' '.join([token for token in tokens if token not in stop_words])  # NLTK 불용어 제거\n","\n","df['content'] = df['content'].apply(remove_stopwords)\n","df['title'] = df['title'].apply(remove_stopwords)\n","\n","# 8. '##' 제거 (content, title)\n","df['content'] = df['content'].str.replace('##', '', regex=False)\n","df['title'] = df['title'].str.replace('##', '', regex=False)\n","\n","# 9. '기자' 제거 (author 열에서)\n","df['author'] = df['author'].str.replace('기자', '', regex=False).str.strip()\n","\n","# 10. 영어 제거 (content, title, author)\n","df['content'] = df['content'].str.replace(r'[a-zA-Z]', '', regex=True)\n","df['title'] = df['title'].str.replace(r'[a-zA-Z]', '', regex=True)\n","df['author'] = df['author'].str.replace(r'[a-zA-Z]', '', regex=True)\n","\n","# 11. 중복된 데이터 제거 (title 기준)\n","df = df.drop_duplicates(subset=['title'])\n","\n","# 12. 불필요한 공백 제거 (content, title, author)\n","df['content'] = df['content'].str.strip()  # 앞뒤 공백 제거\n","df['content'] = df['content'].str.replace(r'\\s+', ' ', regex=True)  # 다중 공백 단일화\n","df['title'] = df['title'].str.strip()  # 앞뒤 공백 제거\n","df['title'] = df['title'].str.replace(r'\\s+', ' ', regex=True)  # 다중 공백 단일화\n","df['author'] = df['author'].str.strip()  # 앞뒤 공백 제거\n","df['author'] = df['author'].str.replace(r'\\s+', ' ', regex=True)  # 다중 공백 단일화\n","\n","# 13. title에 공백 또는 빈 값만 있을 경우 해당 행 제거\n","df = df[df['title'].str.strip() != '']\n","\n","# 클렌징 완료 후 확인\n","print(df.head(10))\n","\n","# 클렌징된 데이터 저장 (Google Drive의 'MyDrive' 폴더에 저장)\n","df.to_csv('/content/drive/MyDrive/Team2/daum_news_data_cleaned.csv', index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ucyy6Eir8ppg"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOPxOSsMjn+wl3jaiky9jTJ","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
